{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3081cf-f4aa-44c5-bd20-6625525e8671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880df63b-1f65-45ff-9249-75d0b648d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bikedf = pd.read_csv('cleaned_df1.csv', na_filter=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b4315-848f-4707-a000-db6b10144e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where segment is \"ppr\" or \"pr\"\n",
    "filtered_data = df[Model['segment'].isin(['Price-performance recommendation', 'performance recommendation'])]\n",
    "\n",
    "# Check the top values in the product_variant column for the filtered data\n",
    "top_product_variants = filtered_data['product_variant'].value_counts()\n",
    "display(top_product_variants)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34715230-a88d-42cd-bf84-f395a1061c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bikedf[\"Premium\"] = Bikedf[\"product_variant\"].apply(lambda x: 1 if \"Premium\" in x else 0)\n",
    "Bikedf[\"Fahrrad- und E-Bike-Versicherung\"] = Bikedf[\"product_variant\"].apply(lambda x: 1 if \"Fahrrad- und E-Bike-Versicherung\" in x else 0)\n",
    "Bikedf[\"Plus\"] = Bikedf[\"product_variant\"].apply(lambda x: 1 if \"Plus\" in x else 0)\n",
    "Bikedf[\"Superior\"] = Bikedf[\"product_variant\"].apply(lambda x: 1 if \"Superior\" in x else 0)\n",
    "Bikedf[\"Premium 1.0\"] = Bikedf[\"product_variant\"].apply(lambda x: 1 if \"Premium 1.0\" in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8e676-1694-4162-9a94-9a892d3775b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bikedf = Bikedf.drop('product_variant', axis=1)\n",
    "\n",
    "#Bikedf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad5d52-8ae2-47ef-8471-46968d43fda1",
   "metadata": {},
   "source": [
    "# Split the Dataset into Features (X) and Target Variables (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d30f053-02dd-4b87-a1d8-5eb3064fe328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Price' and 'Rating' are your target variables\n",
    "X = Bikedf.drop(['price', 'rating'], axis=1)\n",
    "y = Bikedf['price']\n",
    "#y_rating = Bikedf['rating']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15892b69-de97-4d96-ab7f-90fe510f5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Dataset into Training and Testing Sets for Price Prediction# Encode categorical variables using LabelEncoder\n",
    "#label_encoder = LabelEncoder()\n",
    "#X_encoded_label = X.apply(label_encoder.fit_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074c153-7ec5-4820-b49f-ebefd930af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder for categorical variables with more than two categories\n",
    "categories_to_onehot = ['assistance', 'criteria', 'segment', 'provider', 'location', 'month', 'cost_category']\n",
    "onehot_encoder = OneHotEncoder(drop='first')\n",
    "X_encoded_onehot = pd.concat([X_encoded_label.drop(categories_to_onehot, axis=1), pd.get_dummies(X_encoded_label[categories_to_onehot], drop_first=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88dadfe-45af-457d-be4a-df10b88d7f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data with 80% for training and 20% for testing\n",
    "X_train_price, X_test_price, y_train_price, y_test_price = train_test_split(X, y_price, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8df93f-0c96-4e93-941c-83bc6d364a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train_price.select_dtypes(np.number)\n",
    "X_test_num  = X_test_price.select_dtypes(np.number) \n",
    "\n",
    "X_train_cat = X_train_price[categories_to_onehot]\n",
    "X_test_cat  = X_test_price[categories_to_onehot]\n",
    "\n",
    "X_train_binary_df = X_train_price[[\"Premium\",\"Fahrrad- und E-Bike-Versicherung\", \"Plus\", \"Superior\", \"Premium 1.0\"]]\n",
    "X_test_binary_df  = X_test_price[[\"Premium\",\"Fahrrad- und E-Bike-Versicherung\", \"Plus\", \"Superior\", \"Premium 1.0\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d98e02-303e-459e-93f4-7a43ba7f128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "onehot_encoder = OneHotEncoder(drop='first')\n",
    "onehot_encoder.fit(X_train_cat)\n",
    "\n",
    "# Save the encoder with pickle in the \"../encoders/\" folder\n",
    "encoder = onehot_encoder\n",
    "folder_path = r\"C:\\Users\\priya\\Documents\\IRONHACK\\Final_Project\\Encoder\"\n",
    "# Specify the filename for the saved encoder\n",
    "encoder_path = os.path.join(r\"C:\\Users\\priya\\Documents\\IRONHACK\\Final_Project\\Encoder\", \"onehot_encoder.pkl\")\n",
    "# Save the encoder using pickle\n",
    "with open(encoder_path, 'wb') as file:\n",
    "    pickle.dump(encoder, file)\n",
    "\n",
    "X_train_cat_encoded = onehot_encoder.transform(X_train_cat).toarray()\n",
    "X_test_cat_encoded  = onehot_encoder.transform(X_test_cat).toarray()\n",
    "\n",
    "X_train_cat_df = pd.DataFrame(X_train_cat_encoded, columns=onehot_encoder.get_feature_names_out(), index=X_train_cat.index)\n",
    "X_test_cat_df  = pd.DataFrame(X_test_cat_encoded,  columns=onehot_encoder.get_feature_names_out(), index=X_test_cat.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5504c76-c8c0-4f78-816f-4ef14e990215",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_x = PowerTransformer()\n",
    "pt_x.fit(X_train_num)\n",
    "\n",
    "# Save the transfomer in the \"../transfomers/\" folder\n",
    "trained_transformer_object = pt_x\n",
    "# Specify the folder path\n",
    "folder_path = r\"C:\\Users\\priya\\Documents\\IRONHACK\\Final_Project\\Transformer\"\n",
    "# Specify the filename for the saved transformer\n",
    "transformer_filename = \"power_transformer.pkl\"\n",
    "# Specify the full path, including the filename, for saving the transformer\n",
    "transformer_path = os.path.join(r\"C:\\Users\\priya\\Documents\\IRONHACK\\Final_Project\\Transformer\", \"power_transformer.pkl\")\n",
    "# Save the transformer using pickle\n",
    "with open(transformer_path, 'wb') as file:\n",
    "    pickle.dump(trained_transformer_object, file)\n",
    "    \n",
    "X_train_num_trans = pt_x.transform(X_train_num)\n",
    "X_test_num_trans  = pt_x.transform(X_test_num)\n",
    "\n",
    "X_train_num_df = pd.DataFrame(X_train_num_trans, columns=X_train_num.columns, index=X_train_num.index)\n",
    "X_test_num_df  = pd.DataFrame(X_test_num_trans,  columns=X_test_num.columns, index=X_test_num.index)\n",
    "\n",
    "X_train_num_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cfd79-cd84-47a6-ba1d-f4cd45244b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9e6067-a788-48a5-91bc-9611bc0339c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_merged = pd.concat([X_train_num_df, X_train_binary_df, X_train_cat_df], axis=1)\n",
    "X_test_merged  = pd.concat([X_test_num_df, X_test_binary_df, X_test_cat_df], axis=1)\n",
    "\n",
    "display(X_train_merged.head())\n",
    "display(X_test_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454997f-3aab-4745-8e6e-2129cb8e30c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating scaler object\n",
    "scaler = MinMaxScaler()\n",
    "#scaler.fit(X_train)\n",
    "# fit transform on train set\n",
    "X_train_scaled = scaler.fit_transform(X_train_merged)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train_merged.columns)\n",
    "\n",
    "# Save the scaler with pickle in the \"../scalers/\" folder\n",
    "folder_path = r\"C:\\Users\\priya\\Documents\\IRONHACK\\Final_Project\\Scaler\"\n",
    "# Specify the filename for the saved scaler\n",
    "scaler_path = os.path.join(r\"C:\\Users\\priya\\Documents\\IRONHACK\\Final_Project\\Scaler\", \"minmax_scaler.pkl\")\n",
    "# Save the scaler using pickle\n",
    "with open(scaler_path, 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "# transforming also test set\n",
    "X_test_scaled = scaler.transform(X_test_merged)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test_merged.columns)\n",
    "\n",
    "#X_train_trans = scaler.transform(X_train)\n",
    "#X_test_trans  = scaler.transform(X_test)\n",
    "\n",
    "#X_train_trans_df = pd.DataFrame(X_train_trans, columns=X_train.columns, index=X_train.index)\n",
    "#X_test_trans_df  = pd.DataFrame(X_test_trans,  columns=X_test.columns, index=X_test.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87851450-7519-45fe-8012-604d2eb28b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train_scaled_df.head())\n",
    "display(X_test_scaled_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fde48-9df2-4f0f-846c-d3481efcd864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7eb62-f55d-4ee9-a0cd-5974058810e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_trans_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d694caf-a40a-448a-adbb-6b509a62406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = Bikedf['price']\n",
    "y_test = Bikedf['price']\n",
    "# Reshape the y_train to a 2D array\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "y_test = y_test.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490717c-a4d1-425f-b5de-faeabcd5ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "pt_y = PowerTransformer()\n",
    "pt_y.fit(y_train)\n",
    "# Save the transformer in the \"../transformers/\" folder\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = \"Transformer\"\n",
    "\n",
    "# Specify the full path, including the filename, for saving the transformer\n",
    "transformer_path = os.path.join(folder_path, \"power_transformer.pkl\")\n",
    "\n",
    "# Save the transformer using pickle\n",
    "with open(transformer_path, 'wb') as file:\n",
    "    pickle.dump(pt_y, file)\n",
    "   \n",
    "y_train_trans = pt_y.transform(y_train)\n",
    "y_test_trans  = pt_y.transform(y_test)\n",
    "\n",
    "y_train_trans_df = pd.DataFrame(y_train_trans)\n",
    "y_test_trans_df  = pd.DataFrame(y_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c702512d-518e-4df6-840c-01f3db76b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_trans_df.head()\n",
    "y_test_trans_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0ab80-dae6-4225-bebc-495e6872aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_scaled_df.shape, len(y_train))\n",
    "#Match the Number of Samples:\n",
    "y_train = y_train[:39461]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff617dc-eda3-4bd1-b6ed-69eccff98e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict price -> Numerical -> Regression.\n",
    "# LinearRegressor(),  RandomForestRegressor(), GradientBoostingRegressor(), KNNRegressor()\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "\n",
    "# Save your model with pickle in the \"../models/\" folder.\n",
    "folder_path = r\"C:\\Users\\priya\\Documents\\IRONHACK\\Final_Project\\Models\"\n",
    "model_path = os.path.join(folder_path, \"linear_regression_model.pkl\")\n",
    "# Save the model using pickle\n",
    "with open(model_path, 'wb') as file:\n",
    "    pickle.dump(lm, file)\n",
    "    print(\"The new directory is created!\")\n",
    "    \n",
    "# y_pred_train_lm_trans = lm.predict(X_train_trans_df)\n",
    "# y_pred_test_lm_trans  = lm.predict(X_test_trans_df)\n",
    "# y_pred_train_lm = pt_y.inverse_transform(y_pred_train_lm_trans)\n",
    "# y_pred_test_lm  = pt_y.inverse_transform(y_pred_test_lm_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee9a44-26ad-4601-8219-3d270393e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using linear model to get predictions from train and test set\n",
    "\n",
    "y_train_pred = lm.predict(X_train_scaled_df)\n",
    "y_test_pred = lm.predict(X_test_scaled_df)\n",
    "\n",
    "\n",
    "#print(type(y_train))\n",
    "#print(type(X_train_scaled_df))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f756ca-8ef7-4155-8fb3-cb8de99f80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_scaled_df.shape, len(y_train.shape))\n",
    "y_train = Bikedf['price']\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fcae5-4e77-4bfb-ae8e-c140fe36f6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X_train and y_train have the same number of rows\n",
    "X_train = X_train.head(y_train.shape[0])\n",
    "\n",
    "# Ensure X_test and y_test have the same number of rows\n",
    "X_test = X_test.head(y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf753c-b65c-4913-bd1b-a0e06c8ad7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train and y_test to DataFrames\n",
    "y_train_df = pd.DataFrame(y_train, columns=['price'])\n",
    "y_test_df = pd.DataFrame(y_test, columns=['price'])\n",
    "\n",
    "# creating a dictionary comparing train to set predictions\n",
    "results = {\n",
    "    \"Set\": [\"Train\"] * X_train.shape[0] + [\"Test\"] * X_test.shape[0],\n",
    "    \"Real\": list(y_train_df['price'].head(X_train.shape[0])) + list(y_test_df['price'].head(X_test.shape[0])),\n",
    "    \"Predicted\": list(y_train_pred) + list(y_test_pred),\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5559e7-6b7e-456b-b12c-4be6ff14fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary comparing train to set predictions\n",
    "results = {\n",
    "    \"Set\": [\"Train\"] * X_train.shape[0] + [\"Test\"] * X_test.shape[0],\n",
    "    \"Real\": list(y_train.head(X_train.shape[0])) + list(y_test.head(X_test.shape[0])),\n",
    "    \"Predicted\": list(y_train_pred) + list(y_test_pred),\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6fb43d-df07-4684-95b9-d8f330bc84ae",
   "metadata": {},
   "source": [
    "# Train the initial model on the training set for price prediction\n",
    "initial_model_price = RandomForestRegressor(random_state=42)\n",
    "initial_model_price.fit(X_train_price, y_train_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1581fc-507c-4568-b3de-1d2cb13869d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Initial Model for Price Prediction\n",
    "# Make predictions on the test set for price prediction\n",
    "y_pred_price = initial_model_price.predict(X_test_price)\n",
    "\n",
    "# Evaluate the initial model's performance for price prediction\n",
    "initial_mse_price = mean_squared_error(y_test_price, y_pred_price)\n",
    "print(f\"Initial Model MSE for Price Prediction: {initial_mse_price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d84112-11f9-4067-b5d1-34329a9e14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model for price prediction with default hyperparameters\n",
    "model_price = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_price.fit(X_train_price, y_train_price)\n",
    "\n",
    "# Make predictions on the test set for price prediction\n",
    "y_pred_price = model_price.predict(X_test_price)\n",
    "\n",
    "# Evaluate the model's performance for price prediction\n",
    "mse_price = mean_squared_error(y_test_price, y_pred_price)\n",
    "print(f\"Model MSE for Price Prediction: {mse_price}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50969ad-3b64-4245-ac61-381a409c1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a smaller hyperparameter grid\n",
    "param_dist_price = {\n",
    "    'model__n_estimators': [50, 100],\n",
    "    'model__max_depth': [None, 10],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "    'model__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV instead of GridSearchCV\n",
    "random_search_price = RandomizedSearchCV(pipeline_price, param_distributions=param_dist_price,\n",
    "                                          n_iter=5, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)\n",
    "\n",
    "random_search_price.fit(X_price, y_price)\n",
    "\n",
    "# Get the best hyperparameters for price prediction\n",
    "best_params_price_random = random_search_price.best_params_\n",
    "print(f\"Best Hyperparameters for Price Prediction: {best_params_price_random}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3548f-76f2-4baa-be18-70a08dca7af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-Tune Hyperparameters using GridSearchCV for Price Prediction\n",
    "# Define hyperparameter grid for price prediction\n",
    "param_grid_price = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create the model for price prediction\n",
    "model_price = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Grid search for hyperparameter tuning for price prediction\n",
    "grid_search_price = GridSearchCV(model_price, param_grid_price, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_price.fit(X_train_price, y_train_price)\n",
    "\n",
    "# Get the best hyperparameters for price prediction\n",
    "best_params_price = grid_search_price.best_params_\n",
    "print(f\"Best Hyperparameters for Price Prediction: {best_params_price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4603b143-8ad9-4fd8-b168-767d84ee2296",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_price = RandomizedSearchCV(model_price, param_distributions=param_grid_price, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n",
    "random_search_price.fit(X_train_price, y_train_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de0b46-d627-4284-ace5-ed6a9ff0e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Final Model with Best Hyperparameters for Price Prediction\n",
    "# Use the best hyperparameters to train the final model for price prediction\n",
    "final_model_price = RandomForestRegressor(**best_params_price)\n",
    "final_model_price.fit(X_train_price, y_train_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085a11c-7c0c-442c-96ff-f8308266112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Final Model for Price Prediction\n",
    "# Make predictions on the test set using the final model for price prediction\n",
    "y_pred_final_price = final_model_price.predict(X_test_price)\n",
    "\n",
    "# Evaluate the final model's performance for price prediction\n",
    "final_mse_price = mean_squared_error(y_test_price, y_pred_final_price)\n",
    "print(f\"Final Model MSE for Price Prediction: {final_mse_price}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8567cb-da8c-44fe-a024-dfdb8cd2c3fb",
   "metadata": {},
   "source": [
    "# Perform clustering analysis to group customers based on criteria like bike type, assistance, and segment. This can help in tailoring your products to specific customer needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6bf0aa-a66a-47d2-b02e-f31345774f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for clustering (bike type, assistance, and segment)\n",
    "features = ['criteria', 'assistance', 'segment']\n",
    "X = Model[features]\n",
    "\n",
    "# Handle missing values if any\n",
    "X = X.dropna()\n",
    "\n",
    "# Encode categorical variables if necessary\n",
    "X_encoded = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e941cff-d143-4cbd-9ea0-a943decf3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data by scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee6a4d3-a9c7-4466-9e70-84c91cf4bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Elbow method to find the optimal number of clusters\n",
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the within-cluster sum of squares (WCSS) for different values of k\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574fc90b-0606-47f2-adeb-7c4c7fd51a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the desired number of clusters based on the Elbow method (for example, k=3)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Add the predicted cluster labels to the original DataFrame\n",
    "Model['cluster_label'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05809e49-faa4-4f21-a881-90b37a27fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of data points colored by cluster labels\n",
    "plt.scatter(X['criteria'], X['assistance'], c=Model['cluster_label'], cmap='viridis')\n",
    "plt.xlabel('Bike Type')\n",
    "plt.ylabel('Assistance')\n",
    "plt.title('Customer Segmentation')\n",
    "\n",
    "# Add a colorbar to indicate the cluster labels\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Cluster Labels')\n",
    "\n",
    "# Customize the plot with additional options (if needed)\n",
    "# For example, you can adjust the marker size, add gridlines, or modify the axis limits\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8640f68b-8784-4156-ab36-fb7ea4059096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Assuming you have X_train, y_train, X_test, y_test\n",
    "\n",
    "# Initialize KNNRegressor\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Save the model with pickle\n",
    "folder_path = \"Models\"\n",
    "model_path = os.path.join(folder_path, \"knn_regressor_model.pkl\")\n",
    "\n",
    "with open(model_path, 'wb') as file:\n",
    "    pickle.dump(knn_regressor, file)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = knn_regressor.predict(X_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = knn_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Testing RMSE: {test_rmse}\")\n",
    "print(f\"Training R^2: {train_r2}\")\n",
    "print(f\"Testing R^2: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c802e14-d828-4bdc-a16e-36d8d58c79ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
